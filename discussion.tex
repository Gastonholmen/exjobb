\chapter{Results and Discussion}
In this section the final model's capabilities are tested and discussed. Previously, the model has been evaluated on labeled data, thus generating a success rate. Below, the model will make predictions on unlabeled data, and instead output a confidence in its prediction.

First, the model is evaluated on artificial transitions between different surfaces in our dataset. After that, it is tested on real world transitions.

\section{Real and artificial transition regions}

While we up to this point maily have focused on leave-one-out and validation set accuracies we have not in detail studied what the classifier output looks like. We are specifically interested in examining how well performance is when the robot moves across a \emph{transition region}, a region where the surface type changes from grass to nongrass or vice versa. Ideally the prediction should rapidly change according to the new target surface. 

This can be accomplished in two different ways. The first revolves around creating a test set by concatenating the last parts of each session. By doing this, we artificially generate transition region data which we can predict on. Examining these predictions we get a feel for what output the predictor may generate when moving from one surface to the next. The second method is to simply use real-world measurements from when the robot moves across a surface. We should see that at the time point when the robot has reached the transition the prediction change accordingly. 

In the first test, four surfaces are concatenated into a sequence: grass, asphalt, grass, tiles. The predictions are shown in the top part of figure \ref{fig:artificial1}. Below, the predictions are median filtered with filter length $L=5$, see section \ref{detection}. The outliers are removed, rendering a very accurate description of the scene. 

Two more challenging surfaces according to table \ref{tab:loo} are soil and gravel. In figure \ref{fig:artificial2} the predictions on artificial transitions including these are displayed. With median filtering we obtain equally good results. Something worth noting in the two figure is that the surface transition is delayed by two steps after the median filtering. It is important to be aware of what distance this corresponds to in the physical world, since we do not want to detect an edge long after it is passed. Each prediction uses 25 samples, and with a delay of two predictions this means a 50 sample delay. With a sample frequency of 200 Hz, this means a 0.25 second delay in the edge detection, or traveling at the speed $v=0.3$ m/s, a delay corresponding to $0.3\cdot0.25=0.075$ m.

Looking at the high leave-one-out accuracies in table \ref{tab:loo}, the results of the artificial transitions may not come as a surprise, as they are based on the same data. To test the model even further, it will make predictions of real world transitions, not included in our dataset. Figure \ref{fig:trans_tgtg} shows a transition from grass to tiles, and figure \ref{fig:trans_gg} a transition from grass to gravel. Focusing on the median filtered predictions, we see that outside the transition region, the model works well for classifying both materials. What happens around the transition is perhaps more interesting. In the previous figures, the predictions went from 1 to 0 and vice versa in just one step. Here, it takes a while before the model reaches zero-outputs. There are two possible explanations for this. First of all, in the artificial transitions, the surface went from a perfect grass surface to a perfect non-grass surface at an instance. In real transitions, there might be some overlap of grass on the non-grass surface, making the model unsure of what it sees. Another explanation could be that one sensor is facing straight down, whereas the other one looks slightly more forward. Hence, the model is perceiving both grass and tiles simultaneously during a short time. 

The results show great promise, but the work is still at an early stage. The surfaces in the four transitions were intentionally free from as many obstructions as possible. When facing surfaces with characteristics not taken into regard by the model, the results will not likely be as good. 

\begin{figure}
	\centering
	\includegraphics[scale=0.5]{figs_temp/varmats1}
	\caption{Artificial transition region created using samples from four different regions.}
	\label{fig:artificial1}
\end{figure}

\begin{figure}
	\centering
	\includegraphics[scale=0.5]{figs_temp/varmats2}
	\caption{Artificial transition region created using samples from four different regions.}
	\label{fig:artificial2}
\end{figure}

\begin{figure}
	\centering
	\includegraphics[scale=0.5]{figs_temp/transition_grass_tiles_grass}
	\caption{Real-world test where the robot moved across a tile pavement in a lawn.} 
	\label{fig:trans_tgtg}
\end{figure}

\begin{figure}
	\centering
	\includegraphics[scale=0.5]{figs_temp/transition_grass_gravel2}
	\caption{Real-world transition}
\end{figure}

\section{Difficult surfaces}
An often critical aspect of any machine learning system is its performance on dissimilar data. 

An often critical aspect of any machine learning system is its performance on dissimilar data: How well does the algorithn manage when we are faced with new data dissimilar to the ones seen in training? When traversing a lawn of poor quality, are we still able to classify its surface as a grassy one?  

\section{Moisture}

One particularly challenging aspect of adequately classifying surfaces is the ever-changing environmental conditions surronding the sensor. Of particular interest is the moisture content in the surfaces of interest. Greater soil moisture implies higher dielectric constant, which in turn increases radar wave scattering \citep{rappaport_2006}. Thus a single surface may very well change its scattering properties over time. 

% More things that make selecting data tricky

\section{Surface variances}

Such effects is difficult to account for when selecting data. Gather a dataset as diverse as possible

\section{Feature Extraction}
 Machine learning is used to find features it considers are good. By manually performing feat. ext. we remove information that the algorithms may otherwise have used to become even better etc. 

Future work: investigate models with no manual feat. extr.

Table [TABLENAME] is in many ways a very telling one. Here, each sample session was classified without using any of the samples from the session at hand. Instead, all other sessions were used in training. Six different methods of classification were evaluated; two linear and four nonlinear. 

First and foremost it is noted that each tested method performed at the very least \emph{decently}. One may argue that the LSTM model is unstable or that using LDA produces lower accuracy predictions, but they nonetheless generated accuracies above 97.5\%. This remarkable result means that given a random sample from the dataset collected, with no samples from the session the sample was taken from used in training, we can correcly classify the surface as grass or non-grass 39/40 times even with the lower-performing classifiers. With the top performing fully connected model, even higher accuracy was obtained with a low standard deviation. 

