\chapter{Feature selection}

In order to create an effective predictive model one must first select a set of features used to produce each classification. Both a science and an art, this process involves intuition, theory and a hefty amount of trial-and-error. Although a seemingly daunting task, for a model to be effective the feature selection procedure is key, and a carefully considered selection often generates significantly better results than one put together without much thought. 

Ideally, one seeks a small set of variables which accurately captures the information content in some larger set of data. After reducing the number of features into a more manageable format it is then possible to obtain good results with significantly less computational load than if no feature selection had been considered. Furthermore, the feature selection process allows us to pinpoint what data characteristics we wish to monitor, and subsequently which data characteristics we can disregard. 

\section{An impossible modeling problem}

Fundamentally, the radar used transmits pulses with some envelope $A(t)$ of some frequency $\Omega$ into its surroundings.

\begin{equation}
	x(t) = A(t)\sin(\Omega t + \varphi)
\end{equation}

After scattering, a signal $s(t)$ reaches the receiving antenna. This signal is comprised of radiation diffuesly scattered from the target scene. It contains both specular (i.e. mirror-like) and non-specular reflective components. Its content will be a function dependant both on the surrounding surface topography, as well as the dielectric properties of the surface at hand \citep{grossman_popovic_chamberlin_gordon_novotny_2017}. Furthermore, the surfaces at hand have a, to some degree, random structure with varying characteristics. The great difficulty with modeling $s(t)$ is clear - the universe of all possible "realistic" surfaces, both natural and man-made, is almost impossibly large. Without making significant assumptions with regards to surface roughness or dielectric properties, there is no general model of $s(t)$ we can make use of. Instead, we have to come up with a set of \emph{reasonable} features that \emph{should} capture the signal structure. These features will no be based on any specific signal model, but instad rely on the returning signals having stable structure over brief periods of time. Hence our task is to find \emph{defining characteristics} from a given surface using the sequence of one dimensional data obtained from a radar sensor. 


As the radar samples rather quickly at around 200 Hz we can allow ourselves to use a sequence of sweeps to generate one prediction. This allows for feature-processing over a number of sweeps, rather than on a sweep-by-sweep basis. Besides improving the signal-to-noise ratio \citep{w_doerry_2016} we may also investigate time correlations when using multiple radar sweeps. If $T$ sweeps are used per classification, the rate of classifications $F_c$ produced relates to the sampling rate $F_s$ through

\begin{equation}
	F_c = \frac{F_s}{T}
\end{equation} 
The parameter $T$ becomes a tradeoff between accuracy and classification rate. The more samples $T$ used per classification the higher the classification accuracy is, but with the cost of a lower classification frequency. Conversely we may be able to generate feature estimates more rapidly by setting $T$ to a lower value, but will in the process end up with worse feature estimates. 

\section{Features}

The reflectivity of a material, described by its dielectric constant, shows up in the obtained IQ-demodulated sweeps in the amplitude of the returning signals. Therefore it might seem like a good idea to simply measure the energy content of a sweep and use it as a feature. However, due to inconsistencies inbetween individual sensors sweep normalization is performed as a pre-processing step rendering such calculations unusable. We are thus left with investigating the topography of the scene. 

In this section four different features are discussed aimed at capturing the geometric characteristics of a target surface. A radar datapoint captured at time $t$ and range $n$ is denoted as $x(n,t)$. Since we are using $T$ samples per classification, each new set of features $f_m$ are estimated from time $t_m=Tm$ to $Tm+T-1$ 

\subsubsection{Expected signal}

First of all we may characterize a sweep by its envelope form - that is, the shape of the absolute values of the radar sweeps. If we can assume that the samples are taken from some unknown stochastic process $X_{i,t}$ with constant mean over $T$ samples we may for range $i$ and feature index $m$ form the estimate using $T$ consecutive sweeps to form the averaging estimate $s_i(m)$ through


\begin{equation}
	s_i(m) = \E\{X_{i,t_m}\}\frac{1}{T}\sum_{t=0}^{T-1}|x(i, t_m + t)|.
\end{equation}

These estimates form the expected signal feature vector $f_{s,m}$ as 

\begin{equation}
	\mathbf{f}_{s,m} = 
	\begin{bmatrix}
		s_0(m) & s_1(m) & ... & s_{K-1}(m)
	\end{bmatrix}.
\end{equation}


In figure \ref{fig:sweep_average} we see what such an averaging process yields. Individual variances are suppressed to form a stable estimate of the sweep shape. 

\begin{figure}[h]
	\centering
	\includegraphics[scale=0.7]{figs_temp/features/sweep_average}
	\caption{By averaging a number of consecutive sweeps, we reduce noise and make samples more similar. The dashed line shows the average of the solid lines. }
	\label{fig:sweep_average}
\end{figure}

\subsubsection{Autocovariance - range}


We can regard the sequence $\{x(i,t)\}_{t=t_m}^{t_m+T}$ of measurements of range $i$ as samples from a stochastic process $X_{i,t}$ with the autocovariance function

\begin{equation}
	C_{XX}(i,t,s) = \text{Cov}(X_{i,t},X_{i,s}).
\end{equation}

If we then can consider the process to have the following three properties:
\emph{
\begin{enumerate}
	\item Constant finite mean
	\item Autocovariance function only dependant on the difference $(s-t)$ and not on actual values of $s$ and $t$.
	\item Finite variance
\end{enumerate}
}
over the time interval $t_m$ to $t_m+T-1$, it can be regarded as a \emph{Wide-Sense Stationary} (WSS) process \citep{jakobsson_2015}. The first and second assumptions require the returning signal to carry some level of stability, which is reasonble considering the short time-frame $T/F_s$ considered. From the autocovariance function 

\begin{equation}
	r_i(m, k) = \E\big\{(X_{i,t_m} - \E\{X_{i,t_m}\})^*(X_{i,t_m+k} - \E\{X_{i,t_m+k}\})\big\}
\end{equation}

we can then form the (biased) estimated autocovariance function through

\begin{equation}
	\hat{r}_i^b(m, k) = \frac{1}{T}\sum_{t=0}^{T-1-k}\big(x(i,t_m+t) - s_i(m)\big)^*\big(x(i,t_m+t+k) - s_i(m)\big).
\end{equation}


Note that the bias in this estimate is completely inconsequential as all features later are normalized to zero mean unit variance.  Thus, the features formed are

\begin{equation}
	f_{r,mk} = 
	\begin{bmatrix}
		\hat{r}_0(m,k) & \hat{r}_1(m,k) & ... & \hat{r}_{K-1}(m,k)
	\end{bmatrix}
\end{equation}

Noting that the autocovariance sequence at 0 lag produce only real numbers, as any complex number $z = a + bi$ multiplied with its conjugate has zero imaginary part $\text{Im}(z^*z) = \text{Im}((a + bi)(a - bi)) = \text{Im}(a^2 + b^2) = 0$, we may form the full feature component for $q$ autocovariance lags $f_{r,m}$ as

\begin{equation}
	f_{r,m} = 
	\begin{bmatrix}
		f_{r,m,0} & \text{Re}(f_{r,m,1}) & \text{Im}(f_{r,m,1}) & ... & \text{Re}(f_{r,m,q}) & \text{Im}(f_{r,m,q}) 
	\end{bmatrix}
\end{equation}

\begin{figure}[h]
	\centering
	\includegraphics[scale=0.45]{figs_temp/features/autocorr_range.png}
	\caption{The graphs show the autocorrelations computed from time-varying IQ-signals at an appropriate range. The tiled surface, which is the most even, corresponds to the graph with the highest autocorrelation. In contrast, the most uneven surface - grass - is represented by the graph with the lowest autocorrelation.}
	\label{fig:autocorr_range}
\end{figure}


\subsubsection{Autocovariance - energy}

While the sweep normalization process rendered absolute mesurements of signal energy useless, we are still able to investigate its time-dependant structure through the autocovariane function. First we estimate the energy in each sweep $v(t)$ and the average energy $v_a(t)$ in $T$ number of sweeps, and then calculate the real-valued autocorrelation sequence $h(m,k)$. 

\begin{equation}
	v(t) = \frac{1}{N}\sum_{n=0}^{K-1}x(n,t)x^*(n,t)
\end{equation}

\begin{equation} 
	v_a(m) = \frac{1}{T}\sum_{t=0}^{T-1}v(t_m+t)
\end{equation}

\begin{equation}
	h(m,k) = \frac{1}{T}\sum_{t=0}^{T-1-k}\big(v(t_m+t) - v_a(m)\big)^*\big(v(t_m+t+k) - v_a(m)\big)
\end{equation}

As $h(m,k)$ only consist of real values, the energy autocorrelation feature vector $f_{h,m}$ is formed as

\begin{equation}
	f_{h,m} = 
	\begin{bmatrix}
		h(m,0) & h(m,1) & ... & h(m,K-1).
	\end{bmatrix}
\end{equation}

%We can also calculate the autocovariance function of estimated sweep energy. Although the sweeps are normalized in the preprocessing, we can investigate how the sweeps change over time. 

%The average energy in a sweep tells us how much energy is reflected back to the radar. Hence it can be regarded as a measure of how good of a reflector the underlying surface is. The energy depends on the shape of the surface, as well as its dielectric constant. Compared to other materials, grass has a very different surface shape, which potentially gives it a very different reflexivity. However, its dielectric constant could also vary a lot depending on whether it is wet or dry, making it hard to guess its reflective properties.

%By computing the average energy of single sweeps from different surfaces we see in figure \ref{fig:sweep_energy} that grass reflects much less energy. This indicates that the average sweep energy is a good feature for binary grass/not grass classification. However, in order to get a more robust measure of the average energy we do not only compute the average over selected range bins of a single sweep, but we average over a few consecutive sweeps as well. Mathematically, the feature we end up using becomes
%\begin{equation}
%	P(t_m) = \frac{1}{NT}\sum_{t=0}^{T-1}\sum_{n=0}^{K-1}x(n, t_m + t)x^*(n, t_m + t),
%\end{equation}
%where (describe variables)

%Despite the fact that the average energy appears to be a highly relevant feature, it is also much gain dependent, and conflicts with what is stated in section *reference to sweep norm.*. Therefore, if different sensors are to be used for training and future classifications, this feature will not be of any use.




\subsubsection{Fourier Transform}
While traversing a static surface at a constant speed, one could think of it as the details on the surface, such as small rocks or grass, are moving closer to the radar sensor. This affects the transmitted radar waves according to the Doppler effect, and the radar detects \textit{Doppler frequencies}, corresponding to the frequency shifts. Depending on the surface's characteristics, the doppler frequencies may vary. 

To visualize the frequency contents of different measured materials, the discrete Fourier transform is used. In figure \ref{fig:fft}, typical transforms are shown for all materials. The figure is intended to show differences in the DFTs, so to make the difference more clear, the squre root of the DFT has been plotted. For grass (the top two plots), it can be seen that the frequency content is contained mainly at the zero frequency. The other materials contain slightly higher frequency content as well. Gravel, in particular, differs a lot from the other materials with a wide variety of frequency components.

As for the feature vector, a datasquare consisting of $T$ sweeps is selected. Viewing this as a matrix with elements $x(i,t)$, where $i$ denotes fast time, and $t$ slow time, we can compute a $k$-step Fourier transform for each range, $i$, as
\begin{equation}
	\{\mathbb{X}_k^{(i)}\}_{k=0}^{T-1} =  \sum_{n=0}^{T-1}x_{i,n}\exp\Big[-2\pi i\frac{nk}{T}\Big].
\end{equation}
After this, the absolute value of the Fourier transform is computed and the transformations of all ranges in the datasquare are concatenated into a one-dimensional feature vector of the form
\begin{equation}
	\textbf{f}=\begin{bmatrix} \{\mathbb{X}_k^{(0)}\}_{k=0}^{T-1} & \{\mathbb{X}_k^{(1)}\}_{k=0}^{T-1} & \hdots & \{\mathbb{X}_k^{(m)}\}_{k=0}^{T-1} \end{bmatrix},
\end{equation}
where $m$ is the number of considered ranges.


\begin{figure}[h]
	\centering
	\includegraphics[scale=0.4]{figs_temp/features/fft}
	\caption{For each material, a 128-step DFT is computed in slow time for all ranges. The absolute value is taken over the complex DFT, followed by taking the square root of it to make the differences more clear in the plot.}
	\label{fig:fft}
\end{figure}

\rowcolors{2}{gray!25}{white}
\begin{table}
\begin{center}
  \begin{tabular}{|c|cccccc|}
\hline
    \rowcolor{gray!150}
		  & \color{white}\textbf{Abs} & \color{white}\textbf{AC Energy} & \color{white}\textbf{AC Range} & \color{white}\textbf{FFT} & \color{white}\textbf{Features} & \color{white}\textbf{Accuracy} \\
	  Config 1 & X &   &   &   & 28  & 94.49 \\
	  Config 2 & X & X & X & & 175 & 98.49 \\
	  Config 3 & & & & X & 700 & 98.38 \\
\hline
  \end{tabular}
\end{center}
\caption{Feature configurations}
\end{table}


\subsection{Tested feature combinations}

In above section, four different features were described - the average signal shape, the autocovariance in range, the autocovariance in energy and the fourier transform.


