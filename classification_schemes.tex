\chapter{Classification schemes}

In this chapter a few classification models are evaluated for the feature extraction processes described in the preceding chapter. Detailed descriptions of each classification framework is omitted, but good sources for further explanation will be supplied for the interested reader. The aim of this chapter is to compare various models and to determine the most promising one with regards to accuracy, size and complexity. 

\section{Evaluated models}

In this section the effectiveness of a few different models are compared. Two linear models and two non-linear models are evaluated. The two linear models considered, a \emph{Linear Discriminant Analysis} (LDA) and a \emph{Support Vector Machine} (SVM) model.

\subsection{Deep neural networks}

Deep neural networks, or DNN for short, are neural networks that has more than one layer of hidden units between its inputs and outputs \citep{hinton_deng_yu_dahl_mohamed_jaitly_senior_vanhoucke_nguyen_sainath_2012}. 


DNN's in various forms have gained immense popularity over the past two decades achieving considerable success within a wide spectrum of applications, such as in image recognition \citep{szegedy_liu_jia_sermanet_reed_anguelov_erhan_vanhoucke_rabinovich_2018}, acoustic modeling of speech \citep{hinton_deng_yu_dahl_mohamed_jaitly_senior_vanhoucke_nguyen_sainath_2012}, 

\subsection{LSTM}
Mention CNN and LSTM being used for time series classification. Then bring up the article which combines these two to motivate this approach.

Observing one range bin at a time, one could think of the radar data as a time series. This motivates the use of some classification scheme that exploits temporal behaviour. Recurrent neral networks (RNNs) feature this by having feedback within individual layers in the network. \citep{karim_majumdar_darabi_chen_2018} The problem with RNNs, however, is that they suffer from a vanishing or exploding gradient, and can only sustain a short term memory. A way to combat this is to use a neural network layer called long short term memory (LSTM). These are thoroughly described in, for example \citep{hochreiter_schmidhuber_1997}

LSTM-layers have previously been used successfully for classifications in radar applications. For instance in \citep{jithesh_sagayaraj_srinivasa_2018} the method was able to classify flying objects from $\textbf{nnn}$ different classes with an accuracy of ...? In \citep{karim_majumdar_darabi_chen_2018}, the LSTM layer is used in combination with a fully convolutional neural network (FCN), which proves to be a significant improvement from just using FCNs when classifying time series.

% Define block styles
\tikzstyle{decision} = [diamond, draw, fill=blue!20, 
    text width=7.5em, text badly centered, inner sep=0pt]
\tikzstyle{block} = [rectangle, draw, fill=blue!20, 
    text width=7em, text centered, rounded corners, minimum height=4em]
\tikzstyle{line} = [draw, -latex']
\tikzstyle{cloud} = [draw, ellipse,fill=red!20, node distance=3cm,
    minimum height=4em]
\tikzstyle{blockgreen} = [rectangle, draw, fill=green!20, 
    text width=7em, text centered, rounded corners, minimum height=4em]
\tikzstyle{blockbrown} = [rectangle, draw, fill=brown!20, 
    text width=7em, text centered, rounded corners, minimum height=4em]

\begin{figure}
\centering
\begin{tikzpicture}[node distance = 3cm, auto]
	% Place nodes
	\node [blockbrown] (obtained) {Obtained data};
	\node [blockgreen, below left of=obtained] (training) {Training data};
	\node [block, below right of=obtained] (test) {Test data};
	\node [blockgreen, below of=training, node distance=2.2cm] (train pre) {Preprocess and extract features};
	\node [block, below of=test, node distance=2.2cm] (test pre) {Preprocess and extract features};
	\node [blockgreen, below right of=train pre] (est) {Estimate $\mathbf{\mu}_f$, $\mathbf{\sigma}_f$};
	\node [blockgreen, below left of=est] (scale train) {Scale to ZMUV};
	\node [block, below right of=est] (scale test) {Scale to ZMUV};
	\node [cloud, below right of=scale train] (train model) {Train model};
	\node [decision, below of=train model] (classify) {Classification};
	\node [block, below of=classify] (post) {Postprocessing};
	\node [block, below of=post, node distance=2.2cm] (output) {Model output};
	% Draw lines
	\path [line] (obtained) -| (training);
	\path [line] (obtained) -| (test);
	\path [line] (training) -- (train pre);
	\path [line] (test) -- (test pre);
	\path [line, dashed] (train pre) -| (est);
	\path [line,dashed] (est) -- (scale train);
	\path [line,dashed] (est) -- (scale test);
	\path [line] (test pre) -- (scale test);
	\path [line] (train pre) -- (scale train);
	\path [line] (scale train) |- (train model);
	\path [line] (train model) -- (classify);
	\path [line] (scale test) |- (classify);
	\path [line] (classify) -- (post);
	\path [line] (post) -- (output);
\end{tikzpicture}
\caption{Flowchart of classification scheme.}
\end{figure}

\section{Model evaluations}


% Put a common title, "Accuracy", for all models
\rowcolors{2}{gray!25}{white}
\begin{table}
\begin{center}
  \begin{tabular}{|l|l|}
    \rowcolor{blue!35}
Material & DNN \\
pgrass1& 98.25 \\
pgrass2& 88.25 \\
pgrass3& 100.0 \\
pgrass4& 94.7 \\
pgrass5& 97.9 \\
qgrass1& 95.75 \\
qgrass2& 98.4 \\
qgrass3& 99.95 \\
hgrass12& 97.25\\ 
hgrass2 &98.4 \\
hgrass3 &99.65 \\
hgrass4 &99.85 \\
hgrass5 &99.5 \\
hgrass6 &100.0 \\
hgrass7 &98.5 \\
hgrass8 &97.5 \\
hgrass9 &98.65 \\
hgrass10& 95.25\\ 
hgrass11& 96.55\\ 
pasph1 &100.0 \\
pgravel1& 98.15 \\
pasph2 &99.95 \\
pasph3 &99.9 \\
psoil1 &99.9 \\
psoil2 &99.15 \\
ptiles1& 97.8 \\
qasph1& 99.95 \\
qasph2& 100.0 \\
qgravel1& 97.15 \\
qsoil1 &99.9 \\
qsoil2& 94.05 \\
qtiles1& 99.55 \\
rtiles1 &99.9 \\
rgravel1& 99.35 \\
rgravel2 &99.35 \\
rasp1 &99.9 \\
rsoil1 &93.45 \\
G2gravel2& 99.65\\ 
G2gravel1& 99.8 \\
G2soil1 &94.3 \\
G2tiles1& 100.0 \\
hsoil1 &93.4 \\
hsoil2 &95.5
  \end{tabular}
\end{center}
\caption{Feature configurations}
\end{table}

\subsection{Evaluation metrics}

\subsubsection{Validation set accuracy}

\subsubsection{Leave-one-out accuracy}
